# -*- coding: utf-8 -*-
"""Lab11 Scikit-Learn-I.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CLgmnJnV_0Sjs2BRXbVaN_k5wb3fQcDo
"""

# Commented out IPython magic to ensure Python compatibility.
#---------------KNN Algorithm-----------------
from sklearn.datasets import load_breast_cancer 
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import train_test_split 
import matplotlib.pyplot as plt 
# %matplotlib inline
cancer = load_breast_cancer()
print(cancer.DESCR)
print(cancer.feature_names)
print(cancer.target_names)
cancer.data
cancer.data.shape
"""
import pandas as pd 
raw_data=pd.read_csv('breast-cancer-wisconsin-data.csv', delimiter=',') 
raw_data.tail(10)
!pip install mglearn
import mglearn
# mglearn: a library of utility functions the book "Introduction to Machine 
Learning with Python" including scikit-learn functions
mglearn.plots.plot_knn_classification(n_neighbors=3)
"""
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, 
stratify=cancer.target, random_state=42)
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
print('Accuracy of KNN n-5, on the training set: {:.3f}'.format(knn.score(X_train, y_train)))
print('Accuracy of KNN n-5, on the test set: {:.3f}'.format(knn.score(X_test, y_test)))
# Resplit the data, with a different randomization (inspired by Muller & Guido ML book - https://www.amazon.com/dp/1449369413/)
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, 
stratify=cancer.target, random_state=66)
# Create two lists for training and test accuracies
training_accuracy = []
test_accuracy = []
# Define a range of 1 to 10 (included) neighbors to be tested
neighbors_settings = range(1,11)
# Loop with the KNN through the different number of neighbors to determine the most appropriate (best)
for n_neighbors in neighbors_settings:
 clf = KNeighborsClassifier(n_neighbors=n_neighbors)
 clf.fit(X_train, y_train)
 training_accuracy.append(clf.score(X_train, y_train))
 test_accuracy.append(clf.score(X_test, y_test))
# Visualize results - to help with deciding which n_neigbors yields the best results (n_neighbors=6, in this case)
plt.plot(neighbors_settings, training_accuracy, label='Accuracy of the training set')
plt.plot(neighbors_settings, test_accuracy, label='Accuracy of the test set')
plt.ylabel('Accuracy')
plt.xlabel('Number of Neighbors')
plt.legend()

#---------------Logistic Regression---------------------
# Using LogisticRegression on the cancer dataset. Inspired by Muller and Gu ido ML book: (https://www.amazon.com/dp/1449369413/)

from sklearn.datasets import load_breast_cancer 
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
# %matplotlib inline

cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)

log_reg = LogisticRegression() 
log_reg.fit(X_train, y_train)

print('Accuracy on the training subset: {:.3f}'.format(log_reg.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(log_reg.score(X_test, y_test)))
"""
'C':
parameter to control the strength of regularization
lower C => log_reg adjusts to the majority of data points. higher C => correct classification of each data point.
"""

log_reg100 = LogisticRegression(C=100) 
log_reg100.fit(X_train, y_train) 
print('Accuracy on the training subset:{:.3f}'.format(log_reg100.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(log_reg100.score(X_test, y_test)))

log_reg001 = LogisticRegression(C=0.01) 
log_reg001.fit(X_train, y_train) 
print('Accuracy on the training subset:{:.3f}'.format(log_reg001.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(log_reg001.score(X_test, y_test)))



plt.plot(log_reg.coef_.T, 'o', label='C=1') 
plt.plot(log_reg100.coef_.T, '^', label='C=100') 
plt.plot(log_reg001.coef_.T, 'v', label='C=0.01')
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90) 
plt.hlines(0,0, cancer.data.shape[1])	# hlines: horizontal line
plt.ylim(-5,5)	# ylim: y limit value 
plt.xlabel('Coefficient Index')
plt.ylabel('Coefficient Magnitude') 
plt.legend()

#--------------Decision Trees---------------
!pip install mglearn
import mglearn
import numpy as np
# credits to Muller and Guido (https://www.amazon.com/dp/1449369413/) 
import matplotlib.pyplot as plt
# %matplotlib inline

mglearn.plots.plot_tree_not_monotone()
from sklearn.datasets import load_breast_cancer 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)
tree = DecisionTreeClassifier(random_state=0) 
tree.fit(X_train, y_train)

print('Accuracy on the training subset: {:.3f}'.format(tree.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(tree.score(X_test, y_test)))



import graphviz
from sklearn.tree import export_graphviz

export_graphviz(tree, out_file='cancertree.dot', class_names=['malignant', 'b enign'], feature_names=cancer.feature_names,impurity=False, filled=True)

print('Feature importances: {}'.format(tree.feature_importances_)) 
type(tree.feature_importances_)
print(cancer.feature_names) 
n_features = cancer.data.shape[1]
plt.barh(range(n_features), tree.feature_importances_, align='center') 
plt.yticks(np.arange(n_features), cancer.feature_names) 
plt.xlabel('Feature Importance')
plt.ylabel('Feature') 
plt.show()

#---------------Random Forests-------
from sklearn.ensemble import RandomForestClassifier 
from sklearn.model_selection import train_test_split 
from sklearn.datasets import load_breast_cancer 
import matplotlib.pyplot as plt


cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)

forest = RandomForestClassifier(n_estimators=100, random_state=0) 
forest.fit(X_train, y_train)
# n_estimatorsï¼šthe number of trees

print('Accuracy on the training subset: {:.3f}'.format(forest.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(forest.score(X_test, y_test)))

n_features = cancer.data.shape[1]
plt.barh(range(n_features), forest.feature_importances_, align='center') 
plt.yticks(np.arange(n_features), cancer.feature_names) 
plt.xlabel('Feature Importance')
plt.ylabel('Feature') 
plt.show()