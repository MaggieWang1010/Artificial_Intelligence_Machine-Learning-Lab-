# -*- coding: utf-8 -*-
"""Lab12 Scikit-Learn-II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Obm3WjIiBfipmeKfQUE66kEfMe2zy-zA
"""

# Commented out IPython magic to ensure Python compatibility.
#---------------Gradient boosting classifier-----------------

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from google.colab import drive 
drive.mount('/content/drive')
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)
gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)

gbrt.fit(X_train, y_train)

print('The decision function for the 3-class iris dataset:\n\n{}'.format(gbrt.decision_function(X_test[:10])))

print('Predicted probabilities for the samples in the iris dataset:\n\n{}'.format(gbrt.predict_proba(X_test[:10])))

#-------------Preprocessing-------

from sklearn import preprocessing 
import numpy as np

data = np.array([[2.2, 5.9, -1.8], [5.4, -3.2, -5.1], [-1.9, 4.2, 3.2]])
bindata = preprocessing.Binarizer(threshold=1.5).transform(data) 
print('Binarized data:\n\n', bindata)

print('Mean (before)= ', data.mean(axis=0)) 
print('Standard Deviation (before)= ', data.std(axis=0)) 
scaled_data = preprocessing.scale(data)

print('Mean (after)= ', scaled_data.mean(axis=0)) 
print('Standard Deviation (after)= ', scaled_data.std(axis=0))

data
minmax_scaler = preprocessing.MinMaxScaler(feature_range=(0,1)) 
data_minmax = minmax_scaler.fit_transform(data) 
print('MinMaxScaler applied on the data:\n', data_minmax)

data
data_l1 = preprocessing.normalize(data, norm='l1') 
data_l2 = preprocessing.normalize(data, norm='l2')

print('L1-normalized data:\n', data_l1) 
print('\nL2-normalized data:\n', data_l2)

#--------------Label encoding---------------------
import pandas as pd
from IPython.display import display
data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/adult_data.txt', header=None, index_col=False, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])

data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation', 'income']]
display(data)


print('Original Features:\n', list(data.columns), '\n') 
data_dummies = pd.get_dummies(data)
print('Features after One-Hot Encoding:\n', list(data_dummies.columns))

features = data_dummies.loc[:, 'age':'occupation_ Transport-moving']
X = features.values
y = data_dummies['income_ >50K'].values

from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

print('Logistic Regression score on the test set: {:.2f}'.format(logreg.score(X_test, y_test)))

#-------------Automatic Feature Selection---------------
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer 
from sklearn.model_selection import train_test_split 
from sklearn.feature_selection import SelectPercentile 
from sklearn.linear_model import LogisticRegression
# %matplotlib inline
cancer = load_breast_cancer() 
rng = np.random.RandomState(42)
noise = rng.normal(size=(len(cancer.data), 50))

X_w_noise = np.hstack([cancer.data, noise])
X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target, random_state=0, test_size=.5)

select = SelectPercentile(percentile=50) 
select.fit(X_train, y_train)
X_train_selected = select.transform(X_train)

print('X_train.shape is: {}'.format(X_train.shape)) 
print('X_train_selected.shape is: {}'.format(X_train_selected.shape))


mask = select.get_support() 
print(mask)
plt.matshow(mask.reshape(1,-1), cmap='gray_r')


X_test_selected = select.transform(X_test) 
logreg = LogisticRegression() 
logreg.fit(X_train, y_train)
print('The score of Logistic Regression on all features:{:.3f}'.format(logreg.score(X_test, y_test))) 
logreg.fit(X_train_selected, y_train)

print('The score of Logistic Regression on the selected features:{:.3f}'.format(logreg.score(X_test_selected, y_test)))

from sklearn.feature_selection import SelectFromModel 
from sklearn.ensemble import RandomForestClassifier

select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold='median')

select.fit(X_train, y_train)
X_train_s = select.transform(X_train) 
print('The shape of X_train is: ', X_train.shape)
print('The shape of X_train_s is ', X_train_s.shape)

mask = select.get_support() 
plt.matshow(mask.reshape(1,-1), cmap='gray_r') 
plt.xlabel('Index of Features')

X_test_s = select.transform(X_test)
score = LogisticRegression().fit(X_train_s, y_train).score(X_test_s, y_test) 
print('The score of Logistic Regression with the selected features on the test set:{:.3f}'.format(score))